{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer PyTorch实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.数据导入 预处理（分词，建立词表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in d:\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "为 D:\\Anaconda3\\lib\\site-packages\\spacy\\data\\en <<===>> D:\\Anaconda3\\lib\\site-packages\\en_core_web_sm 创建的符号链接\n",
      "\n",
      "    Linking successful\n",
      "    D:\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "    D:\\Anaconda3\\lib\\site-packages\\spacy\\data\\en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n",
      "Requirement already satisfied: de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0 in d:\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "为 D:\\Anaconda3\\lib\\site-packages\\spacy\\data\\de <<===>> D:\\Anaconda3\\lib\\site-packages\\de_core_news_sm 创建的符号链接\n",
      "\n",
      "    Linking successful\n",
      "    D:\\Anaconda3\\lib\\site-packages\\de_core_news_sm -->\n",
      "    D:\\Anaconda3\\lib\\site-packages\\spacy\\data\\de\n",
      "\n",
      "    You can now load the model via spacy.load('de')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data,datasets\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de\n",
    "#下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "#加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "#torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段） \n",
    "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "#此处先设置最大长度20 太长电脑跑不动\n",
    "\n",
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "#filter_pred（callable或None）仅使用filter_pred（example）为True的示例，或使用所有示例（如果为None）\n",
    "MIN_FREQ = 1\n",
    "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']\n"
     ]
    }
   ],
   "source": [
    "print(next(train.trg))\n",
    "#train.trg 是生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273475"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab.freqs[',']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.重写批处理函数，实现按长度分批"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4096\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重写了Iterator的函数\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_mask = src_mask\n",
    "        self.trg_mask = trg_mask\n",
    "        self.ntokens = ntokens\n",
    "        \n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
    "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True)\n",
    "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - \n",
      "[torchtext.data.batch.Batch of size 227 from IWSLT]\n",
      "\t[.src]:[torch.LongTensor of size 9x227]\n",
      "\t[.trg]:[torch.LongTensor of size 18x227]\n",
      "1 - \n",
      "[torchtext.data.batch.Batch of size 186 from IWSLT]\n",
      "\t[.src]:[torch.LongTensor of size 14x186]\n",
      "\t[.trg]:[torch.LongTensor of size 22x186]\n",
      "2 - \n",
      "[torchtext.data.batch.Batch of size 157 from IWSLT]\n",
      "\t[.src]:[torch.LongTensor of size 20x157]\n",
      "\t[.trg]:[torch.LongTensor of size 22x157]\n"
     ]
    }
   ],
   "source": [
    "#看一下valid_iter\n",
    "for i, batch in enumerate(valid_iter):\n",
    "    print(i,'-',batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.创建掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "def make_std_mask(src, tgt, pad):\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "    #type_as 将张量转换为给定类型的张量\n",
    "    return src_mask, tgt_mask\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k = 1).astype('uint8')\n",
    "    #Return a copy of a matrix with the elements below the k-th diagonal zeroed.\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([379, 10])\n",
      "torch.Size([380, 10])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([291, 13])\n",
      "torch.Size([292, 13])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([214, 18])\n",
      "torch.Size([215, 18])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([340, 11])\n",
      "torch.Size([341, 11])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([229, 17])\n",
      "torch.Size([230, 17])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([214, 18])\n",
      "torch.Size([215, 18])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([272, 14])\n",
      "torch.Size([273, 14])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([199, 20])\n",
      "torch.Size([200, 20])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([584, 7])\n",
      "torch.Size([585, 7])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([198, 20])\n",
      "torch.Size([199, 20])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([314, 12])\n",
      "torch.Size([315, 12])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([681, 6])\n",
      "torch.Size([682, 6])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([596, 6])\n",
      "torch.Size([597, 6])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([44, 22])\n",
      "torch.Size([45, 22])\n",
      "--\n",
      "torch.Size([584, 7])\n",
      "torch.Size([585, 7])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([584, 7])\n",
      "torch.Size([585, 7])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([226, 17])\n",
      "torch.Size([227, 17])\n",
      "--\n",
      "torch.Size([584, 7])\n",
      "torch.Size([585, 7])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([239, 16])\n",
      "torch.Size([240, 16])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([421, 9])\n",
      "torch.Size([422, 9])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([255, 15])\n",
      "torch.Size([256, 15])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([203, 18])\n",
      "torch.Size([204, 18])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([190, 21])\n",
      "torch.Size([191, 21])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([463, 8])\n",
      "torch.Size([464, 8])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([203, 19])\n",
      "torch.Size([204, 19])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([362, 11])\n",
      "torch.Size([363, 11])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([584, 7])\n",
      "torch.Size([585, 7])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([360, 11])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([361, 11])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([511, 8])\n",
      "torch.Size([512, 8])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([272, 15])\n",
      "torch.Size([273, 15])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([191, 21])\n",
      "torch.Size([192, 21])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([203, 20])\n",
      "torch.Size([204, 20])\n",
      "--\n",
      "torch.Size([226, 18])\n",
      "torch.Size([227, 18])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([239, 17])\n",
      "torch.Size([240, 17])\n",
      "--\n",
      "torch.Size([371, 11])\n",
      "torch.Size([372, 11])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([214, 18])\n",
      "torch.Size([215, 18])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([255, 16])\n",
      "torch.Size([256, 16])\n",
      "--\n",
      "torch.Size([584, 7])\n",
      "torch.Size([585, 7])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([194, 21])\n",
      "torch.Size([195, 21])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([454, 9])\n",
      "torch.Size([455, 9])\n",
      "--\n",
      "torch.Size([435, 9])\n",
      "torch.Size([436, 9])\n",
      "--\n",
      "torch.Size([185, 22])\n",
      "torch.Size([186, 22])\n",
      "--\n",
      "torch.Size([408, 10])\n",
      "torch.Size([409, 10])\n",
      "--\n",
      "torch.Size([340, 12])\n",
      "torch.Size([341, 12])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n",
      "torch.Size([291, 14])\n",
      "torch.Size([292, 14])\n",
      "--\n",
      "torch.Size([314, 13])\n",
      "torch.Size([315, 13])\n",
      "--\n",
      "torch.Size([214, 19])\n",
      "torch.Size([215, 19])\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#探究掩码\n",
    "for i, batch in enumerate(train_iter):\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    src_mask = (src != pad_idx).unsqueeze(-2)\n",
    "    \n",
    "    #unsqueeze 增加一维\n",
    "    tgt_mask = (trg != pad_idx).unsqueeze(-2)\n",
    "   #print(Variable(subsequent_mask(trg.size(-1)).type_as(tgt_mask.data)).size())\n",
    "    \n",
    "    tgt_mask = tgt_mask & Variable(subsequent_mask(trg.size(-1)).type_as(tgt_mask.data))\n",
    "    #print(tgt_mask)\n",
    "    #print((trg[1:] != pad_idx).data.sum())\n",
    "    #统计trg中除bos，以及blank外词的个数\n",
    "    print(trg[1:].size())\n",
    "    print(trg.size())\n",
    "    print('--')\n",
    "    #print(src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 13521,    11,  ...,   175,     5,     3],\n",
      "        [    2,    62,   309,  ...,  1548,     5,     3],\n",
      "        [    2,   802,    54,  ...,    15,     5,     3],\n",
      "        ...,\n",
      "        [    2,   344,    80,  ...,    34,    16,    31],\n",
      "        [    2,   802,    54,  ...,    13,   308,    31],\n",
      "        [    2,   111, 27949,  ...,    33,    31,    21]])\n",
      "tensor([[    2, 13521,    11,  ...,     5,     3,     1],\n",
      "        [    2,    62,   309,  ...,     5,     3,     1],\n",
      "        [    2,   802,    54,  ...,     5,     3,     1],\n",
      "        ...,\n",
      "        [    2,   344,    80,  ...,    16,    31,     3],\n",
      "        [    2,   802,    54,  ...,   308,    31,     3],\n",
      "        [    2,   111, 27949,  ...,    31,    21,     3]])\n"
     ]
    }
   ],
   "source": [
    "print(trg[:, :-1])\n",
    "print(trg)\n",
    "#为什么model.forward中,带入trg的句子都把最后一个词给去掉\n",
    "#删除的这个词必定是ENS 或者是 pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([227, 18, 18])\n",
      "torch.Size([227, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "print(tgt_mask.size())\n",
    "print(tgt_mask[:, :-1, :-1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.建立整个模型框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义标准的编码器-解码器框架\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"take in and process masked src and tgt sequences\"\n",
    "        memory = self.encoder(self.src_embed(src), src_mask)\n",
    "        output = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        mean = x.mean(-1, keepdim=True)#求mean最后一个维度的均值，并保持维度不变\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2*(x - mean)/(std + self.eps) + self.b_2\n",
    "#归一化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of two sublayers, self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size,self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "        \n",
    "    def forward(self, x,memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask = None, dropout = 0.0):\n",
    "    \"scaled dot product attention\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0,-1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "   #print('--')\n",
    "    #print(p_attn)\n",
    "    p_attn = F.dropout(p_attn, p = dropout)\n",
    "    return torch.matmul(p_attn, value), p_attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout = 0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model//h\n",
    "        self.h = h\n",
    "        self.p = dropout\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        #1）do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h,self.d_k).transpose(1, 2) for l, x in \n",
    "                             zip(self.linears, (query, key, value))]\n",
    "        # 依次取出每一个数组的元素然后进行组合\n",
    "        #2) Apply attention on all the projected vectors in batch\n",
    "        x, self.attn = attention (query, key, value, mask = mask, dropout = self.p)\n",
    "        #3)\"Concat\" using a view and apply a final linear\n",
    "        x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#逐位置的前馈网络  编码器和解码器模块最后都包含一个全连接的前馈网络，独立相同的应用于每一个位置\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model,d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings,self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0.,max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * \n",
    "                            -(math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, x.size(1)], requires_grad = False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #softmax 实现单词生成\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator,self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型整体,将以上模块组合\n",
    "def make_model(src_vocab, tgt_vocab, N = 6, d_model = 512, d_ff = 2048, h = 8, dropout = 0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model = make_model(10, 10, 1)\n",
    "tmp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.定义学习率，误差计算，进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义学习率\n",
    "class NoamOpt:\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        #批数，倍率，预热度，梯度方式\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size **(-0.5) *\n",
    "            min(step ** (-0.5),step * self.warmup**(-1.5)))\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "                    torch.optim.Adam(model.parameters(), lr=0,betas=(0.9, 0.98),eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义标签平滑\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义误差计算\n",
    "def loss_backprop(generator, criterion, out, targets, normalize):\n",
    "    \"\"\"\n",
    "    Memory optmization. Compute each timestep separately and sum grads.\n",
    "    \"\"\"\n",
    "    assert out.size(1) == targets.size(1)\n",
    "    total = 0.0\n",
    "    out_grad = []\n",
    "    for i in range(out.size(1)):\n",
    "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
    "        gen = generator(out_column)\n",
    "        loss = criterion(gen, targets[:, i]) / normalize\n",
    "        total += loss.item()\n",
    "        loss.backward()\n",
    "        out_grad.append(out_column.grad.data.clone())\n",
    "    out_grad = torch.stack(out_grad, dim=1)\n",
    "    out.backward(gradient=out_grad)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "                        \n",
    "        model_opt.step()\n",
    "        model_opt.optimizer.zero_grad()\n",
    "        if i % 10 == 1:\n",
    "            print(i, loss, model_opt._rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
    "    #model.test()\n",
    "    model.train()\n",
    "    total = 0\n",
    "\n",
    "    for i,batch in enumerate(valid_iter):\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens)\n",
    "        #以下改动\n",
    "        model_opt.step()\n",
    "        model_opt.optimizer.zero_grad()\n",
    "        print(i, loss, model_opt._rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  app.launch_new_instance()\n",
      "D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8.699741840362549 6.987712429686844e-07\n",
      "11 8.635714590549469 4.192627457812107e-06\n",
      "21 8.430009961128235 7.686483672655528e-06\n",
      "31 8.183791875839233 1.118033988749895e-05\n",
      "41 8.078345537185669 1.4674196102342371e-05\n",
      "51 8.033951759338379 1.8168052317185794e-05\n",
      "61 7.96922379732132 2.1661908532029216e-05\n",
      "71 7.8630282282829285 2.515576474687264e-05\n",
      "81 6.87393993139267 2.8649620961716057e-05\n",
      "91 7.799618691205978 3.214347717655948e-05\n",
      "101 7.556484550237656 3.56373333914029e-05\n",
      "111 7.386423751711845 3.913118960624633e-05\n",
      "121 7.070801317691803 4.262504582108975e-05\n",
      "131 6.652575820684433 4.611890203593317e-05\n"
     ]
    }
   ],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "model_opt = get_std_opt(model)\n",
    "\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "#criterion.cuda()\n",
    "#for epoch in range(5):\n",
    "    #train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)\n",
    "    #valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)\n",
    "    \n",
    "#这里为了节约时间仅仅对验证集训练一次\n",
    "train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.利用贪心解码实现翻译，并对翻译进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encoder(model.src_embed(src), src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decoder(Variable(model.tgt_embed(ys)), memory, src_mask, Variable(subsequent_mask(ys.size(1)).type_as(src_mask.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        #为什么我觉得应该取第i个词\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation:\tMarcus monetary kill monetary Dutchman Dutchman British Jeffery DropBox Dutchman Dutchman Dutchman sampled sampled Jeffery Jeffery Jeffery Jeffery Jeffery Jeffery Jeffery Jeffery Jeffery DropBox drawn drawn Jeffery Jeffery tin tin tin tin tin tin tin tin tin kill bebop kill kill kill kill kill frightens kill kill kill kill kill kill kill DropBox DropBox DropBox DropBox DropBox DropBox DropBox \n",
      "Target:\tSo what I did , I planted a food forest in front of my house . \n",
      "Translation:\tCyber Cyber Howard fabled fabled fabled fabled fabled fabled fabled fabled fabled affirmation geometrically geometrically fabled fabled fabled fabled fabled cruelly Kapital Kapital Kapital Kapital Kapital Kapital Kapital bundle bundle RNA tin tin tin fabled bundle bundle bundle bundle bundle kill kill fabled fabled fabled affirmation affirmation affirmation affirmation affirmation kill Cyber Cyber Cyber affirmation affirmation affirmation affirmation affirmation \n",
      "Target:\tYou can see the hubs , like who are the leaders in the group . \n",
      "Translation:\tHoward Howard Howard Howard Howard Diaz Diaz Smog telepistemology telepistemology telepistemology burqa burqa burqa burqa telepistemology satchidananda satchidananda recorders Petrie recorders neutrality neutrality neutrality recognize drawn telepistemology telepistemology telepistemology telepistemology telepistemology recognize leap telepistemology telepistemology telepistemology Lovelaces xenophobic xenophobic Petrie kill spalls complaint complaint complaint Diaz Diaz telepistemology telepistemology recorders publicly Howard Diaz Diaz Diaz Diaz recognize recognize resulted \n",
      "Target:\tAnd the first answer is , \" I do n't know , they do n't put me in charge of that . \" \n",
      "Translation:\tweekday Howard Howard Howard Howard weekday weekday weekday weekday timbers timbers timbers Edutainment Edutainment complaint complaint Edutainment assayed sassy Shackleton Shackleton Shackleton Shackleton complaint timbers timbers Cuddy Edutainment complaint complaint complaint complaint complaint complaint sassy sassy complaint weekday weekday weekday complaint assayed assayed assayed assayed assayed assayed assayed assayed assayed papyrus papyrus DropBox DropBox DropBox DropBox DropBox weekday weekday \n",
      "Target:\tFor four decades Gaddafi 's tyrannical regime destroyed the infrastructure as well as the culture and the moral fabric of Libyan society . \n",
      "Translation:\tChelsea Howard committee Howard triumph triumph triumph flatlined flatlined flatlined sleek Edutainment burqa Edutainment Edutainment curious tiresome tiresome Shackleton Shackleton Shackleton Shackleton Shackleton Shackleton Shackleton Edutainment Edutainment Shackleton Shackleton Shackleton Shackleton tiresome tiresome tiresome bebop bebop bebop bebop bebop pubic pubic kill kill Edutainment Edutainment Edutainment triumph triumph triumph triumph triumph retrain heterosexual heterosexual triumph triumph triumph sites sites \n",
      "Target:\tI was just three years old when my brother came along , and I was so excited that I had a new being in my life . \n",
      "Translation:\trandomize Howard randomize randomize Howard randomize offender wriggle xenophobic essences nobleman xenophobic Edutainment contraception contraception contraception contraception aberration aberration aberration aberration neutrality neutrality neutrality locust locust locust locust Cooking aberration aberration aberration aberration aberration bebop bebop bebop bebop bebop bebop contraception bebop outdated assayed aberration Edutainment assayed labor labor aberration aberration aberration aberration bowmaking Cooking bebop Howard Howard nobleman \n",
      "Target:\tOur first project , the one that has inspired my first book , \" <unk> from the <unk> , \" was a project where we Italians decided to teach Zambian people how to grow food . \n",
      "Translation:\taerosolized contraception contraception contraception contraception contraception cognition contraception contraception contraception bowmaking contraception bowmaking contraception contraception contraception bowmaking contraception bowmaking bowmaking bowmaking contraception bowmaking contraception bowmaking contraception contraception contraception contraception bowmaking contraception bowmaking contraception contraception contraception contraception contraception contraception contraception contraception contraception contraception bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking contraception bowmaking bowmaking \n",
      "Target:\tBut I cast my mind back to the things that they 've taught me about individuality and communication and love , and I realize that these are things that I would n't want to change with normality . \n",
      "Translation:\tundertone taken taken taken taken taken taken taken taken Blicket Blicket Blicket 1369 outboard mops mops taken bowmaking bowmaking tiresome TMJ TMJ Helicopter Helicopter Helicopter Helicopter Workbench Workbench builders builders TMJ TMJ builders builders builders builders builders builders builders Acheulian builders disturb Polaroid complaint Blicket Blicket Workbench Workbench Sathi Sathi Sathi bowmaking bowmaking bowmaking Workbench Workbench carnivalito carnivalito carnivalito \n",
      "Target:\tNow there are a lot of wonderful things that you could make art about in Afghanistan , but personally I do n't want to paint rainbows ; I want to make art that disturbs identity and challenges authority and exposes hypocrisy and reinterprets reality and even uses kind of an imaginative ethnography to try and understand the world that we live in . \n",
      "Translation:\t32-piece Workbench Math Hello taken taken taken wild wild controversially wild toolshed toolshed wild wild controversially entertain entertain Azra Azra Azra barricading barricading barricading barricading Workbench Workbench Workbench Workbench Polaroid Polaroid ballet ballet complaint complaint Polaroid Polaroid Polaroid Polaroid Polaroid toolshed toolshed Minute Polaroid barricading barricading barricading barricading bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking bowmaking Minute barricading barricading \n",
      "Target:\tSo if the photographer is right there and the light is right there , like a nice <unk> , and the client says , \" Cameron , we want a walking shot , \" well then this leg goes first , nice and long , this arm goes back , this arm goes forward , the head is at three quarters , and you just go back and forth , just do that , and then you look back at your imaginary friends , 300 , 400 , 500 times . \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "list_tr = []\n",
    "list_tg = []\n",
    "for i, batch in enumerate(valid_iter):\n",
    "    src = batch.src.transpose(0, 1)[:1]\n",
    "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
    "    out = greedy_decode(model, src, src_mask,  max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
    "    list_trans = []\n",
    "    list_tgt = []\n",
    "    print(\"Translation:\", end=\"\\t\")\n",
    "    for i in range(1, out.size(1)):\n",
    "        sym = TGT.vocab.itos[out[0, i]]\n",
    "        if sym == \"</s>\": break\n",
    "        list_trans.append(sym)\n",
    "        print(sym, end =\" \")\n",
    "    list_tr.append(list_trans)\n",
    "    print()\n",
    "    print(\"Target:\", end=\"\\t\")\n",
    "    for i in range(1, batch.trg.size(0)):\n",
    "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
    "        if sym == \"</s>\": break\n",
    "        list_tgt.append(sym)\n",
    "        print(sym, end =\" \")\n",
    "    list_tg.append(list_tgt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n",
      "torch.Size([1, 1, 9])\n",
      "torch.LongTensor\n",
      "torch.Size([1, 1, 14])\n",
      "torch.LongTensor\n",
      "torch.Size([1, 1, 18])\n",
      "torch.LongTensor\n",
      "torch.Size([1, 1, 24])\n",
      "torch.LongTensor\n",
      "torch.Size([1, 1, 29])\n",
      "torch.LongTensor\n",
      "torch.Size([1, 1, 34])\n",
      "torch.LongTensor\n",
      "torch.Size([1, 1, 42])\n",
      "torch.LongTensor\n",
      "torch.Size([1, 1, 68])\n",
      "torch.LongTensor\n",
      "torch.Size([1, 1, 90])\n"
     ]
    }
   ],
   "source": [
    "#调试\n",
    "for i, batch in enumerate(valid_iter):\n",
    "    src = batch.src.transpose(0, 1)[:1]#这里只测试第一个句子\n",
    "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
    "    #print(src.size())\n",
    "    #print(src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(TGT.vocab.stoi[\"<s>\"]).type_as(src.data)\n",
    "    tgtmask = Variable(subsequent_mask(ys.size(1)).type_as(src_mask.data))\n",
    "    #.type_as(src.data)\n",
    "    #\n",
    "    print(ys.type())\n",
    "    print(src_mask.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用bleu对翻译进行评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list_tr)):\n",
    "    reference = [list_tg[i]]\n",
    "    candidate = list_tr[i]\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
